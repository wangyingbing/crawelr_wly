from DrissionPage import WebPage
from DataRecorder import Recorder
from tqdm import tqdm
import time
import random
from datetime import datetime, date, timedelta
from bs4 import BeautifulSoup
import pandas as pd
import re
from pymongo import MongoClient
from pymongo.errors import DuplicateKeyError, BulkWriteError

# ------------------------- MongoDB配置 -------------------------
MONGO_URI = "mongodb://localhost:27017/"
DATABASE_NAME = "xiaohongshu"
COLLECTION_NAME = "note_details"


def connect_mongodb():
    """创建MongoDB连接并智能处理索引"""
    try:
        client = MongoClient(MONGO_URI)
        db = client[DATABASE_NAME]
        collection = db[COLLECTION_NAME]

        # 获取当前所有索引
        existing_indexes = collection.index_information()
        target_field = "笔记链接"
        new_index_name = "note_url_unique_index"

        # 清理旧索引（如果有）
        old_indexes = [name for name, idx in existing_indexes.items()
                       if idx['key'][0][0] == target_field and name != new_index_name]

        for old_name in old_indexes:
            print(f"正在清理旧索引：{old_name}")
            collection.drop_index(old_name)

        # 创建新索引（如果不存在）
        if new_index_name not in existing_indexes:
            collection.create_index([(target_field, 1)],
                                    unique=True,
                                    name=new_index_name)
            print("唯一索引创建成功")

        return collection

    except Exception as e:
        print(f"MongoDB连接失败：{str(e)}")
        return None


def save_to_mongodb(data_list):
    """批量存储数据到MongoDB（自动去重）"""
    if not data_list:
        return

    try:
        collection = connect_mongodb()
        # 修复点：使用is None进行判断
        if collection is None:
            print("MongoDB连接失败，跳过数据存储")
            return

        # 数据预处理
        processed_data = []
        for data in data_list:
            # 转换日期类型
            if isinstance(data['采集日期'], date):
                data['采集日期'] = datetime.combine(data['采集日期'], datetime.min.time())
            # 转换标签为字符串
            data['标签'] = [str(tag) for tag in data['标签']]
            processed_data.append(data)

        # 批量插入（忽略重复数据）
        try:
            result = collection.insert_many(processed_data, ordered=False)
            print(f"成功插入 {len(result.inserted_ids)} 条新数据")
        except BulkWriteError as e:
            # 计算实际插入数量
            inserted = e.details['nInserted']
            errors = len(e.details['writeErrors'])
            print(f"跳过 {errors} 条重复数据，MongoDB成功插入 {inserted} 条新数据")

    except Exception as e:
        print(f"数据存储异常：{type(e).__name__} - {str(e)}")



def validate_url(url):
    """验证URL格式是否合法"""
    return url.startswith(('http://', 'https://'))


def sign_in():
    flag = True
    sign_in_page = WebPage()
    while True:
        sign_in_page.get('https://www.xiaohongshu.com')
        sign_in_page.set.window.max()
        time.sleep(3)
        search_placeholder = sign_in_page.active_ele.html
        if '输入手机号' in search_placeholder:
            if flag:
                print("请扫码登录#")
                flag = False
            else:
                time.sleep(20)
        else:
            print("登录成功！")
            sign_in_page.change_mode()
            break


def record_error_link_to_txt(link, error_type='page_missing'):
    with open('error_links.txt', 'a') as file:
        file.write(f"{error_type}: {link}\n")
    print(f'【错误记录】{error_type}: {link}')


def open_url(url):
    global page
    if not validate_url(url):
        raise ValueError(f"Invalid URL: {url}")

    page = WebPage('s')
    try:
        page.get(url)
        if page.title == '滑块验证':
            page.change_mode()
            page.get(url)
            print('请手动处理验证码')
        elif '你访问的页面不见了' in page.title:
            record_error_link_to_txt(url, 'page_not_found')
            return False
        return True
    except Exception as e:
        record_error_link_to_txt(url, f'other_error({str(e)})')
        return False


def get_author_info(page):
    try:
        div_author = page.ele('.author-container', timeout=2)
        div_info = div_author.ele('.info', timeout=2)
        author_name = div_info.ele('.username', timeout=2).text
        author_link = div_info.eles('tag:a')[0].link.split("?")[0]
        return {'author_name': author_name, 'author_link': author_link}
    except Exception as e:
        print(f"获取作者信息失败: {str(e)}")
        return {'author_name': '', 'author_link': ''}


def parse_xhs_time(date_str):
    """将小红书时间格式转换为标准日期"""
    today = datetime.now()
    date_str = date_str.strip()

    # 处理IP属地
    ip_match = re.search(r'([\u4e00-\u9fa5]{2,})$', date_str)
    if ip_match:
        ip_location = ip_match.group()
        date_str = date_str.replace(ip_location, '').strip()
    else:
        ip_location = ''

    # 解析时间
    if '昨天' in date_str:
        date_obj = today - timedelta(days=1)
        time_part = re.search(r'\d{1,2}:\d{2}', date_str).group()
        hour, minute = map(int, time_part.split(':'))
        return date_obj.replace(hour=hour, minute=minute), ip_location
    elif '小时前' in date_str:
        hours_ago = int(re.search(r'\d+', date_str).group())
        return today - timedelta(hours=hours_ago), ip_location
    else:
        try:
            return datetime.strptime(date_str, '%Y-%m-%d %H:%M'), ip_location
        except:
            try:
                return datetime.strptime(date_str, '%Y-%m-%d'), ip_location
            except:
                return None, ip_location


def get_note_content(page):
    content = {
        'note_warning_tip': '',
        'note_title': '',
        'note_desc': '',
        'tags': [],
        'note_date': None,
        'ip_location': ''
    }

    try:
        note_container = page.ele('.note-container', timeout=2)
        content['note_warning_tip'] = note_container.ele('.note-warning-tip', timeout=0).text
    except:
        pass

    try:
        note_content = page.ele('.note-content', timeout=2)
        content['note_title'] = note_content.ele('.title', timeout=0).text
        content['note_desc'] = note_content.ele('.desc', timeout=0).text
        content['tags'] = [tag.texts()[0] for tag in note_content.eles('.tag', timeout=0)]

        raw_date_str = note_content.ele('.bottom-container', timeout=0).text
        parsed_date, ip_location = parse_xhs_time(raw_date_str)
        content['note_date'] = parsed_date
        content['ip_location'] = ip_location

    except Exception as e:
        print(f"获取内容失败: {str(e)}")
    return content


def get_count(page):
    try:
        html = page.html
        soup = BeautifulSoup(html, 'html.parser')
        return {
            'like_count': soup.find('meta', attrs={'name': 'og:xhs:note_like'})['content'],
            'collect_count': soup.find('meta', attrs={'name': 'og:xhs:note_collect'})['content'],
            'chat_count': soup.find('meta', attrs={'name': 'og:xhs:note_comment'})['content']
        }
    except Exception as e:
        print(f"获取统计信息失败: {str(e)}")
        return {'like_count': '0', 'collect_count': '0', 'chat_count': '0'}


# ------------------------- 采集主逻辑 -------------------------
def get_note_page_info(url, data_buffer):
    if not validate_url(url):
        print(f"跳过无效URL: {url}")
        return

    try:
        if not open_url(url):
            return

        author_info = get_author_info(page)
        content = get_note_content(page)
        count = get_count(page)

        note_info = {
            '采集日期': date.today(),
            '作者': author_info['author_name'],
            '笔记标题': content['note_title'],
            '发布日期': content['note_date'],
            '点赞数': count['like_count'],
            '收藏数': count['collect_count'],
            '评论数': count['chat_count'],
            'IP属地': content['ip_location'],
            '提示': content['note_warning_tip'],
            '笔记链接': url,
            '作者链接': author_info['author_link'],
            '标签': content['tags'],
            '笔记内容': content['note_desc']
        }

        print(f"【采集成功】{url}")
        data_buffer.append(note_info)

    except Exception as e:
        print(f"处理失败: {url} - {str(e)}")
        record_error_link_to_txt(url, f'process_error({str(e)})')
    finally:
        time.sleep(random.uniform(1, 2))


def read_urls_from_txt(path):
    with open(path, 'r') as file:
        return [line.strip() for line in file if line.strip() and validate_url(line.strip())]


if __name__ == '__main__':
    sign_in()

    # 初始化数据记录
    current_time = time.strftime("%Y-%m-%d %H%M%S")
    note_info_excel_path = f'小红书笔记详情{current_time}.xlsx'
    r = Recorder(path=note_info_excel_path, cache_size=10)  # 增大缓存减少IO

    # 读取待采集URL
    note_urls_file_path = r"C:\Users\Lenovo\Desktop\URL.txt"
    note_urls = read_urls_from_txt(note_urls_file_path)
    print(f'有效URL数量: {len(note_urls)}')

    # 数据采集缓冲区
    data_buffer = []
    batch_size = 20  # 每采集20条保存一次

    # 执行采集
    for i, url in enumerate(tqdm(note_urls, desc="采集进度")):
        get_note_page_info(url, data_buffer)

        # 批量保存
        if (i + 1) % batch_size == 0 or (i + 1) == len(note_urls):
            # 保存到Excel
            if data_buffer:
                r.add_data(data_buffer)
                r.record()

            # 保存到MongoDB
            save_to_mongodb(data_buffer)

            # 清空缓冲区
            data_buffer.clear()

    # 最终保存
    if data_buffer:
        r.add_data(data_buffer)
        r.record()
        save_to_mongodb(data_buffer)

    # Excel去重处理
    try:
        df = pd.read_excel(note_info_excel_path, engine='openpyxl')
        df = df.drop_duplicates(subset=['笔记链接'])
        df.to_excel(note_info_excel_path, index=False, engine='openpyxl')
        print(f"Excel文件已保存：{note_info_excel_path}")
    except Exception as e:
        print(f"Excel处理失败：{str(e)}")
