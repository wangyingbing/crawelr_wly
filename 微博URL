import datetime
import time
import random
import pandas as pd
import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# 配置参数
KEYWORDS = "就业"
START_DATE = '2025-03-10'
END_DATE = '2025-03-11'
MAX_RETRIES = 3
REQUEST_DELAY = (1, 3)
PAGES_PER_DAY = 50
COOKIE = ("//个人微博cookie")

class WeiboScraper:
    def __init__(self):
        self.session = requests.Session()
        self.ua = UserAgent()
        self._init_session()

    def _init_session(self):
        """初始化带重试机制的会话"""
        retry = Retry(
            total=MAX_RETRIES,
            backoff_factor=0.3,
            status_forcelist=(500, 502, 503, 504)
        )
        adapter = HTTPAdapter(max_retries=retry)
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)

    def _get_headers(self):
        """生成随机请求头"""
        return {
            'User-Agent': self.ua.random,
            'Cookie': COOKIE,
            'Referer': 'https://weibo.com/'
        }

    def _random_delay(self):
        """随机延迟防止封禁"""
        time.sleep(random.uniform(*REQUEST_DELAY))

    def generate_dates(self):
        """生成日期范围"""
        start = datetime.datetime.strptime(START_DATE, "%Y-%m-%d")
        end = datetime.datetime.strptime(END_DATE, "%Y-%m-%d")
        return [start + datetime.timedelta(days=i) for i in range((end - start).days + 1)]

    def parse_page(self, html):
        """解析页面内容"""
        soup = BeautifulSoup(html, 'lxml')

        if soup.find('div', class_='card card-no-result'):
            print("触发反爬限制或没有更多数据")
            return None, True

        cards = soup.find_all('div', class_='card-wrap')
        if not cards:
            print("未找到有效数据卡片")
            return None, False

        results = []
        for card in cards:
            try:
                item = {
                    'author': self._parse_author(card),
                    'content': self._parse_content(card),
                    'reposts': self._parse_interaction(card, -3),
                    'comments': self._parse_interaction(card, -2),
                    'likes': self._parse_likes(card),
                    'time': self._parse_time(card),
                    'link': self._parse_link(card)
                }
                results.append(item)
            except Exception as e:
                print(f"解析失败: {str(e)}")
                continue
        return results, False

    def _parse_author(self, card):
        """解析发布者信息"""
        author_tag = card.find('a', {'class': 'name'})
        return author_tag.text.strip() if author_tag else None

    def _parse_content(self, card):
        """解析微博正文"""
        content_tag = card.find('p', class_='txt')
        if not content_tag:
            return None
        if content_tag.find('a', string='展开全文'):
            return content_tag.text.replace('展开全文', '').strip()
        return content_tag.text.strip()

    def _parse_interaction(self, card, index):
        """解析互动数据"""
        actions = card.find_all('a', class_='woo-box-flex')
        if len(actions) >= abs(index):
            text = actions[index].text.strip()
            return self._convert_number(text)
        return 0

    def _parse_likes(self, card):
        """解析点赞数"""
        likes_tag = card.find('span', class_='woo-like-count')
        return self._convert_number(likes_tag.text.strip()) if likes_tag else 0

    def _parse_time(self, card):
        """解析发布时间"""
        from_tag = card.find('p', class_='from')
        if not from_tag:
            return None
        time_tag = from_tag.find('a', recursive=False)
        return time_tag.text.strip() if time_tag else None

    def _parse_link(self, card):
        """解析微博链接"""
        from_tag = card.find('p', class_='from')
        if not from_tag:
            return None
        time_tag = from_tag.find('a', recursive=False)
        if time_tag and time_tag.has_attr('href'):
            href = time_tag['href']
            if not href.startswith('http'):
                # 已修正拼写错误
                return f'https://weibo.com{href}'
            return href
        return None

    def _convert_number(self, text):
        """转换数字格式"""
        if '万' in text:
            return int(float(text.replace('万+', '').replace('万', '')) * 10000)
        return int(text) if text.isdigit() else 0

    def scrape(self):
        """主爬取流程"""
        all_data = []
        dates = self.generate_dates()

        for date in dates:
            print(f"\n开始爬取 {date.strftime('%Y-%m-%d')} 的数据...")
            date_str = date.strftime("%Y-%m-%d")

            for page in range(1, PAGES_PER_DAY + 1):
                self._random_delay()
                print(f"正在处理第 {page} 页...", end='\r')

                try:
                    url = f"https://s.weibo.com/weibo?q={KEYWORDS}&typeall=1&suball=1&timescope=custom:{date_str}-0:{date_str}-23&page={page}"
                    response = self.session.get(url, headers=self._get_headers(), timeout=15)
                    response.raise_for_status()

                    data, blocked = self.parse_page(response.text)
                    if blocked:
                        break
                    if data:
                        all_data.extend(data)
                        print(f"已采集 {len(all_data)} 条数据", end='\r')

                except Exception as e:
                    print(f"请求失败: {str(e)}")
                    continue

        self.save_data(all_data)

    def save_data(self, data):
        """保存数据到Excel"""
        if not data:
            print("没有需要保存的数据")
            return

        df = pd.DataFrame(data)
        df = df.drop_duplicates(subset=['content'])
        df = df[df['content'].notna()]
        df['time'] = pd.to_datetime(df['time'], errors='coerce')

        filename = f"{KEYWORDS}_微博数据_{datetime.datetime.now().strftime('%Y%m%d%H%M')}.xlsx"
        df.to_excel(filename, index=False, engine='openpyxl')
        print(f"\n数据已保存至 {filename}")


if __name__ == '__main__':
    scraper = WeiboScraper()
    scraper.scrape()
