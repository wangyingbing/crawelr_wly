from DrissionPage import WebPage
from DataRecorder import Recorder
from tqdm import tqdm
import time
import random
from datetime import datetime, date
from bs4 import BeautifulSoup
import pandas as pd
import re
import os


def validate_url(url):
    """严格验证知乎URL格式"""
    patterns = [
        r'^https://www\.zhihu\.com/question/\d+[/?]?.*',
        r'^https://zhuanlan\.zhihu\.com/p/\d+[/?]?.*',
        r'^https://www\.zhihu\.com/answer/\d+[/?]?.*'
    ]
    return any(re.match(p, url) for p in patterns)


def sign_in(page):
    """知乎登录处理"""
    page.get('https://www.zhihu.com/signin')
    page.set.window.max()

    try:
        if page.ele('text=登录', timeout=3):
            print("请完成登录操作...")
            while True:
                if page.url.startswith('https://www.zhihu.com/'):
                    print("登录成功!")
                    return
                time.sleep(5)
    except:
        print("检测到已登录状态")


def extract_content_data(page):
    """提取主要内容数据（含智能分割日期和IP）"""
    content = {
        'title': page.title.split(' - 知乎')[0],
        'content': '',
        'publish_date': '',
        'vote_count': '0',
        'comment_count': '0',
        'ip_location': ''
    }

    try:
        # 统一处理时间和IP信息
        time_ele = page.ele('.ContentItem-time', timeout=3)
        if time_ele:
            raw_text = time_ele.text.strip()

            # 使用正则表达式匹配日期和IP
            pattern = re.compile(
                r'发布于\s*?(?P<date>\d{4}-\d{1,2}-\d{1,2})[^·]*·?\s*IP\s*属地\s*[:：]?\s*(?P<ip>\S+)'
            )
            match = pattern.search(raw_text)

            if match:
                content['publish_date'] = match.group('date')
                content['ip_location'] = match.group('ip')
            else:
                # 处理无IP的纯日期情况
                date_match = re.search(r'\d{4}-\d{1,2}-\d{1,2}', raw_text)
                if date_match:
                    content['publish_date'] = date_match.group()
                else:
                    content['publish_date'] = raw_text.replace('发布于', '').strip()

        # 提取内容
        if '/p/' in page.url:  # 专栏文章
            content['title'] = page.ele('.Post-Title').text
            content['content'] = page.ele('.Post-RichText').text

        elif '/answer/' in page.url:  # 回答
            content['title'] = page.ele('.QuestionHeader-title').text
            content['content'] = page.ele('.RichContent-inner').text

        # 获取互动数据
        soup = BeautifulSoup(page.html, 'html.parser')
        content['vote_count'] = soup.find('meta', itemprop='upvoteCount')['content']
        content['comment_count'] = soup.find('meta', itemprop='commentCount')['content']

    except Exception as e:
        print(f"内容解析失败: {str(e)}")

    return content


def process_zhihu_page(url, page):
    """处理单个知乎页面"""
    try:
        page.get(url)
        time.sleep(random.uniform(1, 3))

        # 处理安全验证
        if '安全验证' in page.title:
            print('需要人工验证...')
            input("完成验证后按回车继续...")
            page.get(url)  # 重新加载页面

        # 检查内容是否存在
        if page.ele('text=你似乎来到了没有知识存在的荒原', timeout=2):
            return {'error': '内容不存在'}

        content_data = extract_content_data(page)

        return {
            '采集日期': date.today().strftime('%Y-%m-%d'),
            '标题': content_data['title'],
            '文章链接': url,
            '内容': content_data['content'][:2000],  # 限制内容长度
            '赞同数': content_data['vote_count'],
            '评论数': content_data['comment_count'],
            '发布日期': content_data['publish_date'],
            'IP地址': content_data['ip_location']
        }

    except Exception as e:
        print(f"页面处理异常: {str(e)}")
        return {'error': str(e)}


def main():
    # 初始化浏览器
    page = WebPage()
    recorder = None

    try:
        # 获取路径
        desktop = os.path.join(os.path.expanduser("~"), "Desktop")
        project_dir = os.path.dirname(os.path.abspath(__file__))

        # 登录处理
        sign_in(page)

        # 生成输出文件
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        excel_name = f"知乎采集结果_{timestamp}.xlsx"
        error_log = f"采集错误_{timestamp}.txt"

        excel_path = os.path.join(project_dir, excel_name)
        error_path = os.path.join(project_dir, error_log)
        url_file = os.path.join(desktop, "URL.txt")

        # 初始化记录器
        recorder = Recorder(excel_path, cache_size=1)

        # 读取URL文件
        if not os.path.exists(url_file):
            raise FileNotFoundError(f"桌面未找到URL.txt文件，请确保文件存在")

        with open(url_file, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if validate_url(line.strip())]

        print(f"有效URL数量: {len(urls)}")

        # 开始采集
        progress = tqdm(urls, desc='采集进度')
        for url in progress:
            result = process_zhihu_page(url, page)
            if 'error' not in result:
                recorder.add_data(result)
                progress.set_postfix({'最新': url[:30]})
            else:
                with open(error_path, 'a', encoding='utf-8') as f:
                    f.write(f"{datetime.now()}\t{url}\t{result['error']}\n")
            time.sleep(random.uniform(1, 3))

    except Exception as e:
        print(f"程序异常: {str(e)}")
    finally:
        # 确保数据写入
        if recorder:
            recorder.record()
        page.quit()

        # 去重处理
        if os.path.exists(excel_path):
            try:
                df = pd.read_excel(excel_path)
                orig_count = len(df)
                df.drop_duplicates(subset=['文章链接'], inplace=True)
                df.to_excel(excel_path, index=False)
                print(f"去重完成: {orig_count} -> {len(df)} 条数据")
                print(f"文件路径: {os.path.abspath(excel_path)}")
            except Exception as e:
                print(f"去重失败: {str(e)}")


if __name__ == '__main__':
    main()
